{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de7d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting liwc\n",
      "  Downloading liwc-0.5.0-py2.py3-none-any.whl (5.1 kB)\n",
      "Installing collected packages: liwc\n",
      "Successfully installed liwc-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e8279c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import liwc\n",
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1501831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_categories(lines):\n",
    "    \"\"\"\n",
    "    Read (category_id, category_name) pairs from the categories section.\n",
    "    Each line consists of an integer followed a tab and then the category name.\n",
    "    This section is separated from the lexicon by a line consisting of a single \"%\".\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == \"%\":\n",
    "            return\n",
    "        # ignore non-matching groups of categories\n",
    "        if \"\\t\" in line:\n",
    "            category_id, category_name = line.split(\"\\t\", 1)\n",
    "            yield category_id, category_name\n",
    "\n",
    "\n",
    "def _parse_lexicon(lines, category_mapping):\n",
    "    \"\"\"\n",
    "    Read (match_expression, category_names) pairs from the lexicon section.\n",
    "    Each line consists of a match expression followed by a tab and then one or more\n",
    "    tab-separated integers, which are mapped to category names using `category_mapping`.\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        parts = line.split(\"\\t\")\n",
    "        yield parts[0], [category_mapping[category_id] for category_id in parts[1:]]\n",
    "\n",
    "\n",
    "def read_dic(filepath):\n",
    "    \"\"\"\n",
    "    Reads a LIWC lexicon from a file in the .dic format, returning a tuple of\n",
    "    (lexicon, category_names), where:\n",
    "    * `lexicon` is a dict mapping string patterns to lists of category names\n",
    "    * `category_names` is a list of category names (as strings)\n",
    "    \"\"\"\n",
    "    with open(filepath, encoding='utf_8_sig') as lines:\n",
    "        # read up to first \"%\" (should be very first line of file)\n",
    "        for line in lines:\n",
    "            if line.strip() == \"%\":\n",
    "                break\n",
    "        # read categories (a mapping from integer string to category name)\n",
    "        category_mapping = dict(_parse_categories(lines))\n",
    "        # read lexicon (a mapping from matching string to a list of category names)\n",
    "        lexicon = dict(_parse_lexicon(lines, category_mapping))\n",
    "    return lexicon, list(category_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "510c56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, category_names = read_dic('../data/Traditional_Chinese_LIWC2015_Dictionary_v1.5.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ddeb9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function (Function Words)',\n",
       " 'pronoun (Pronouns)',\n",
       " 'ppron (Personal Pronouns)',\n",
       " 'i (I)',\n",
       " 'we (We)',\n",
       " 'you (You)',\n",
       " 'shehe (SheHe)',\n",
       " 'they (They)',\n",
       " 'youpl (youpl)',\n",
       " 'ipron (Impersonal Pronouns)',\n",
       " 'prep (Prepositions)',\n",
       " 'auxverb (Auxiliary Verbs)',\n",
       " 'adverb (Adverbs)',\n",
       " 'conj (Conjunctions)',\n",
       " 'negate (Negations)',\n",
       " 'quanunit (quanunit)',\n",
       " 'prepend (prepend)',\n",
       " 'specart (specart)',\n",
       " 'particle (particle)',\n",
       " 'modal_pa (modal_pa)',\n",
       " 'general_pa (general_pa)',\n",
       " 'compare (Comparisons)',\n",
       " 'interrog (Interrogatives)',\n",
       " 'number (Numbers)',\n",
       " 'quant (Quantifiers)',\n",
       " 'tensem (tensem)',\n",
       " 'focuspast (Past Focus)',\n",
       " 'focuspresent (Present Focus)',\n",
       " 'focusfuture (Future Focus)',\n",
       " 'progm (progm)',\n",
       " 'affect (Affect)',\n",
       " 'posemo (Positive Emotions)',\n",
       " 'negemo (Negative Emotions)',\n",
       " 'anx (Anx)',\n",
       " 'anger (Anger)',\n",
       " 'sad (Sad)',\n",
       " 'social (Social)',\n",
       " 'family (Family)',\n",
       " 'friend (Friends)',\n",
       " 'female (Female)',\n",
       " 'male (Male)',\n",
       " 'cogproc (Cognitive Processes)',\n",
       " 'insight (Insight)',\n",
       " 'cause (Causal)',\n",
       " 'discrep (Discrepancies)',\n",
       " 'tentat (Tentative)',\n",
       " 'certain (Certainty)',\n",
       " 'differ (Differentiation)',\n",
       " 'percept (Perceptual Processes)',\n",
       " 'see (See)',\n",
       " 'hear (Hear)',\n",
       " 'feel (Feel)',\n",
       " 'bio (Biological Processes)',\n",
       " 'body (Body)',\n",
       " 'health (Health)',\n",
       " 'sexual (Sexual)',\n",
       " 'ingest (Ingest)',\n",
       " 'drives (Drives)',\n",
       " 'affiliation (Affiliation)',\n",
       " 'achieve (Achievement)',\n",
       " 'power (Power)',\n",
       " 'reward (Reward)',\n",
       " 'risk (Risk)',\n",
       " 'relativ (Relativity)',\n",
       " 'motion (Motion)',\n",
       " 'space (Space)',\n",
       " 'time (Time)',\n",
       " 'work (Work)',\n",
       " 'leisure (Leisure)',\n",
       " 'home (Home)',\n",
       " 'money (Money)',\n",
       " 'relig (Religion)',\n",
       " 'death (Death)',\n",
       " 'informal (Informal Language)',\n",
       " 'swear (Swear)',\n",
       " 'netspeak (Netspeak)',\n",
       " 'assent (Assent)',\n",
       " 'nonflu (Nonfluencies)',\n",
       " 'filler (Filler Words)']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d7830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f733cf",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d67b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/sentiment_analysis_latest.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2acdcb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3267\n",
       "negative    1943\n",
       "positive     959\n",
       "Name: sentiment_label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fbf007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_tag = {'positive':2, 'neutral':1, 'negative':0}\n",
    "df['sentiment_label']= df['sentiment_label'].replace(replace_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43432d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3267\n",
       "0    1943\n",
       "2     959\n",
       "Name: sentiment_label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e3032",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da05e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Harrison\\Downloads\\2023_07_19_supplemet_material\\2023_07_19_supplemet_material\\data\\tokenization_dict\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\Harrison\\AppData\\Local\\Temp\\jieba.u92fdb9fc4f964cbd4d35e3368f83bac9.cache\n",
      "Loading model cost 0.869 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# load self-defined tokenization dictionary\n",
    "jieba.set_dictionary('../data/tokenization_dict/dict.txt')\n",
    "jieba.load_userdict('../data/tokenization_dict/hk_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da17fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep Chinese char, English words and numbers, and then remove stopwords\n",
    "speech_list = list(df['msg_replace'])\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a\\u4e00-\\u9fa5]\")\n",
    "speech_list = [list(jieba.cut(rule.sub('', speech))) for speech in speech_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a9c3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['msg_token'] = speech_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0af0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e284270",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(category for token in gettysburg_tokens for category in parse(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a56491b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m liwc \u001b[38;5;241m=\u001b[39m[] \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg_token\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m     gettysburg_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposemo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m      5\u001b[0m     liwc\u001b[38;5;241m.\u001b[39mappend(gettysburg_counts)\n\u001b[0;32m      6\u001b[0m liwc_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(liwc)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\collections\\__init__.py:577\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(iterable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\collections\\__init__.py:670\u001b[0m, in \u001b[0;36mCounter.update\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(iterable)\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m liwc \u001b[38;5;241m=\u001b[39m[] \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg_token\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m     gettysburg_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(Counter(category \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m item \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m category \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposemo\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m      5\u001b[0m     liwc\u001b[38;5;241m.\u001b[39mappend(gettysburg_counts)\n\u001b[0;32m      6\u001b[0m liwc_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(liwc)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "#LIWC Features Extraction\n",
    "liwc =[] \n",
    "for item in df['msg_token']:\n",
    "    gettysburg_counts = list(Counter(category for token in item for category in parse(token) if category == 'posemo').items())\n",
    "    liwc.append(gettysburg_counts)\n",
    "liwc_ = np.array(liwc)\n",
    "liwc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44b6bcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "gettysburg_counts = Counter(category for token in gettysburg_tokens for category in parse(token))\n",
    "print(gettysburg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "'posemo (Positive Emotions)',\n",
    " 'negemo (Negative Emotions)',\n",
    "'anx (Anx)',\n",
    " 'anger (Anger)',\n",
    " 'sad (Sad)',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9042675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['msg_token_str'] = [' '.join(i) for i in df['msg_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a19d0827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name 你好 呀 今日 有 咩 想 同 我哋 傾 呀'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['msg_token_str'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1f32e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # you may want to use a smarter tokenizer\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "487a795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven years ago our fathers brought forth on this continent a new nation conceived in liberty and dedicated to the proposition that all men are created equal now we are engaged in a great civil war testing whether that nation or any nation so conceived and so dedicated can long endure we are met on a great battlefield of that war we have come to dedicate a portion of that field as a final resting place for those who here gave their lives that that nation might live it is altogether fitting and proper that we should do this\n"
     ]
    }
   ],
   "source": [
    "gettysburg = '''Four score and seven years ago our fathers brought forth on\n",
    "  this continent a new nation, conceived in liberty, and dedicated to the\n",
    "  proposition that all men are created equal. Now we are engaged in a great\n",
    "  civil war, testing whether that nation, or any nation so conceived and so\n",
    "  dedicated, can long endure. We are met on a great battlefield of that war.\n",
    "  We have come to dedicate a portion of that field, as a final resting place\n",
    "  for those who here gave their lives that that nation might live. It is\n",
    "  altogether fitting and proper that we should do this.'''.lower()\n",
    "gettysburg_tokens = tokenize(gettysburg)\n",
    "print(*gettysburg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ffee6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object tokenize at 0x00000211B7D0C900>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gettysburg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "546d3158",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m我的\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "parse('我的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c53a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
